'''
1. Initialize random values for weights and bias

2. Select epochs , loss function and learning rate value and optimizer which will decide the 
   formula for weight update ( for example - w new = w old - n * grad at w old)

3. Find the gradient w.r.t loss and w and b at each epoch

4. Update parameters (optimizer)
   new params = old params - n * gradient at old param

5. Repeat for all epochs also can apply early stopping if loss is same not changing
   that means new param = old param and grad =0/very small


'''

class Grad:
    pass